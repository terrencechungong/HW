{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "tar_gz_path = f\"{current_path}/20_newsgroups.tar.gz\"  \n",
    "\n",
    "\n",
    "with tarfile.open(tar_gz_path, mode=\"r:gz\") as tar:\n",
    "    tar.extractall(path=\"newsgroup_data\")\n",
    "\n",
    "print(\"Extraction complete.\")\n",
    "\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    \n",
    "    processed_lines = lines[4:]\n",
    "\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        file.writelines(processed_lines)\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(\"newsgroup_data\"):\n",
    "    topic = root.split('/')[-1]\n",
    "    os.makedirs(f\"./newsgroup_data/20_newsgroups_test/{topic}\", exist_ok=True)\n",
    "    halfway_mark = len(files) // 2\n",
    "    for index, file in enumerate(files):\n",
    "        file_path = os.path.join(root, file)\n",
    "        process_file(file_path)\n",
    "        if index >= halfway_mark:\n",
    "        \n",
    "            new_file_path = os.path.join(f\"./newsgroup_data/20_newsgroups_test/{topic}\", file)\n",
    "            shutil.move(file_path, new_file_path)\n",
    "\n",
    "        print(f\"Processed: {file_path}\")\n",
    "\n",
    "print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(word: str):\n",
    "    word = word.lower()\n",
    "    return word.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "# init  hashmap of type word_freq[class] = {word: count}\n",
    "word_freq = {} # {class:defaultdict(int)}\n",
    "# init overall freq counter (vocabulary) to filter {word: count}\n",
    "vocabulary = defaultdict(int) # {word:int}\n",
    "# for topic in topics: LOGIC HERE\n",
    "parent_folder = f\"{current_path}/newsgroup_data/20_newsgroups/\"  \n",
    "seen_first = False\n",
    "TOTAL_WORD_COUNT_FOR_CLASS = 'TOTAL_WORD_COUNT_FOR_CLASS/TOPIC'\n",
    "\n",
    "for root, dirs, files in os.walk(parent_folder):\n",
    "    if not seen_first:\n",
    "        seen_first = True\n",
    "        continue\n",
    "    topic = root.split('/')[-1]\n",
    "    print(topic)\n",
    "    word_freq[topic] = defaultdict(int)\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        \n",
    "        # Open and read each file\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "            # Split content into words\n",
    "            words = content.split()\n",
    "            \n",
    "            # Iterate through each word in the file\n",
    "            for word in words:\n",
    "                # Remove punctuation from the word\n",
    "                cleaned_word = remove_punctuation(word)\n",
    "                word_freq[topic][cleaned_word] += 1\n",
    "                word_freq[topic][TOTAL_WORD_COUNT_FOR_CLASS] += 1\n",
    "                vocabulary[cleaned_word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_300 = set()\n",
    "vocab = sorted(vocabulary.items(), key=lambda x: x[1], reverse=True)\n",
    "index = 1\n",
    "topics = set()\n",
    "for word, count in vocab:\n",
    "    if index <= 300:\n",
    "        top_300.add(word)\n",
    "        index += 1\n",
    "        continue\n",
    "    for topic in word_freq.keys():\n",
    "        word_freq[topic][word] += 1\n",
    "\n",
    "for topic in word_freq.keys():\n",
    "    topics.add(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 100\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Step 9: Use the function to prepare data\u001b[39;00m\n\u001b[0;32m     99\u001b[0m dataset_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./newsgroup_data/20_newsgroups/\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Update with your dataset path\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m tfidf_matrix, label_vectors, classes \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_for_svm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Now tfidf_matrix is a 2D NumPy array where each row is a document's TF-IDF vector\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# label_vectors is a dictionary containing labels for each class\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 91\u001b[0m, in \u001b[0;36mprepare_data_for_svm\u001b[1;34m(dataset_dir)\u001b[0m\n\u001b[0;32m     88\u001b[0m idf \u001b[38;5;241m=\u001b[39m compute_idf(df, total_docs)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Compute TF-IDF feature matrix\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_tfidf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Generate labels for one-vs-all classification\u001b[39;00m\n\u001b[0;32m     94\u001b[0m label_vectors \u001b[38;5;241m=\u001b[39m create_one_vs_all_labels(labels, classes)\n",
      "Cell \u001b[1;32mIn[10], line 65\u001b[0m, in \u001b[0;36mcompute_tfidf\u001b[1;34m(tokenized_docs, word_index, idf)\u001b[0m\n\u001b[0;32m     63\u001b[0m     tf \u001b[38;5;241m=\u001b[39m tf \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(doc_tokens)  \u001b[38;5;66;03m# Normalize TF by document length\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     tfidf \u001b[38;5;241m=\u001b[39m tf \u001b[38;5;241m*\u001b[39m idf\n\u001b[1;32m---> 65\u001b[0m     tfidf_matrix[doc_idx] \u001b[38;5;241m=\u001b[39m tfidf\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tfidf_matrix\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# one class will be one. all else will be -1\n",
    "# Step 1: Load documents and preprocess\n",
    "def load_documents(dataset_dir):\n",
    "    documents = []\n",
    "    labels = []\n",
    "    classes = sorted(os.listdir(dataset_dir))  # Get folder names (class labels) sorted for consistency\n",
    "    \n",
    "    for class_name in classes:\n",
    "        class_folder = os.path.join(dataset_dir, class_name)\n",
    "        if os.path.isdir(class_folder):\n",
    "            for doc_name in os.listdir(class_folder):\n",
    "                doc_path = os.path.join(class_folder, doc_name)\n",
    "                with open(doc_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                    text = file.read()\n",
    "                    documents.append(text)\n",
    "                    labels.append(class_name)\n",
    "    return documents, labels, classes\n",
    "\n",
    "# Step 2: Tokenization and Preprocessing (with stop words and punctuation removal)\n",
    "def tokenize_and_preprocess(doc):\n",
    "    tokens = doc.split()  # Simple tokenization; consider using a tokenizer for better results\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        clean_token = remove_punctuation(token)\n",
    "        if clean_token and clean_token not in top_300:\n",
    "            processed_tokens.append(clean_token)\n",
    "    return processed_tokens\n",
    "\n",
    "# Step 3: Build the vocabulary\n",
    "def build_vocabulary(tokenized_docs):\n",
    "    vocabulary = set()\n",
    "    for doc_tokens in tokenized_docs:\n",
    "        vocabulary.update(doc_tokens)\n",
    "    vocabulary = sorted(vocabulary)  # Sort the vocabulary to maintain consistent order\n",
    "    word_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    return vocabulary, word_index\n",
    "\n",
    "# Step 4: Calculate DF for all terms\n",
    "def compute_df(tokenized_docs, word_index):\n",
    "    df = np.zeros(len(word_index))\n",
    "    for doc_tokens in tokenized_docs:\n",
    "        unique_words = set(doc_tokens)\n",
    "        for word in unique_words:\n",
    "            idx = word_index[word]\n",
    "            df[idx] += 1\n",
    "    return df\n",
    "\n",
    "# Step 5: Calculate IDF\n",
    "def compute_idf(df, total_docs):\n",
    "    idf = np.log((total_docs) / (1 + df))  # Adding 1 to avoid division by zero\n",
    "    return idf\n",
    "\n",
    "# Step 6: Compute TF-IDF for each document\n",
    "def compute_tfidf(tokenized_docs, word_index, idf):\n",
    "    total_docs = len(tokenized_docs)\n",
    "    tfidf_matrix = np.zeros((total_docs, len(word_index)))\n",
    "    \n",
    "    for doc_idx, doc_tokens in enumerate(tokenized_docs):\n",
    "        tf = np.zeros(len(word_index))\n",
    "        for word in doc_tokens:\n",
    "            idx = word_index[word]\n",
    "            tf[idx] += 1\n",
    "        tf = tf / len(doc_tokens)  # Normalize TF by document length\n",
    "        tfidf = tf * idf\n",
    "        tfidf_matrix[doc_idx] = tfidf\n",
    "    return tfidf_matrix\n",
    "\n",
    "# Step 7: Generate One-vs-All Labels for SVM\n",
    "def create_one_vs_all_labels(labels, classes):\n",
    "    label_vectors = {}\n",
    "    for class_name in classes:\n",
    "        label_vector = np.array([1 if label == class_name else -1 for label in labels])\n",
    "        label_vectors[class_name] = label_vector\n",
    "    return label_vectors\n",
    "\n",
    "# Step 8: Main function to create feature matrix and labels\n",
    "def prepare_data_for_svm(dataset_dir):\n",
    "    # Load and preprocess documents\n",
    "    print('loading doc')\n",
    "    documents, labels, classes = load_documents(dataset_dir)\n",
    "    print('loaded doc')\n",
    "    tokenized_docs = [tokenize_and_preprocess(doc) for doc in documents]\n",
    "    print('done')\n",
    "    total_docs = len(documents)\n",
    "    \n",
    "    print('built vocab')\n",
    "    # Build vocabulary and word index mapping\n",
    "    vocabulary, word_index = build_vocabulary(tokenized_docs)\n",
    "    print('done building vocab')\n",
    "    # Compute DF and IDF\n",
    "    df = compute_df(tokenized_docs, word_index)\n",
    "    print('computed df')\n",
    "    idf = compute_idf(df, total_docs)\n",
    "    print('done with idf')\n",
    "    \n",
    "    # Compute TF-IDF feature matrix\n",
    "    tfidf_matrix = compute_tfidf(tokenized_docs, word_index, idf)\n",
    "    print('got tfidf')\n",
    "    \n",
    "    # Generate labels for one-vs-all classification\n",
    "    label_vectors = create_one_vs_all_labels(labels, classes)\n",
    "    \n",
    "    return tfidf_matrix, label_vectors, classes\n",
    "\n",
    "# Step 9: Use the function to prepare data\n",
    "dataset_dir = './newsgroup_data/20_newsgroups/'  # Update with your dataset path\n",
    "tfidf_matrix, label_vectors, classes = prepare_data_for_svm(dataset_dir)\n",
    "\n",
    "# Now tfidf_matrix is a 2D NumPy array where each row is a document's TF-IDF vector\n",
    "# label_vectors is a dictionary containing labels for each class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-apply",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
